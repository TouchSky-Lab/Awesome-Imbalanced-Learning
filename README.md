# 1 Survey
- Learning from imbalanced data | IEEE TKDE, 2009| 
- Learning from imbalanced data: open challenges and future directions | 2016 |
- Learning from class-imbalanced data: Review of methods and applications | 2017
- A systematic study of the class imbalance problem in convolutional neural networks | 2018
- Survey on deep learning with class imbalance | 2019

# 2 Ensemble Learning
## 2.1 General ensemble
- Self-paced Ensemble | ICDE 2020 |
- MESA: Boost Ensemble Imbalanced Learning with MEta-SAmpler (NeurIPS 2020)
- Exploratory Undersampling for Class-Imbalance Learning (IEEE Trans. on SMC, 2008)

## 2.2 Boosting-based method
- AdaBoost: Adaptive Boosting with C4.5 | 1995
- DataBoost:  Boosting with Data Generation for Imbalanced Data  | 2004
- SMOTEBoost: Synthetic Minority Over-sampling TEchnique Boosting
- MSMOTEBoost: Modified Synthetic Minority Over-sampling TEchnique Boosting
- RAMOBoost: Ranked Minority Over-sampling in Boosting
- RUSBoost: Random Under-Sampling Boosting
- AdaBoostNC:  Adaptive Boosting with Negative Correlation Learning
- EUSBoost: Evolutionary Under-sampling in Boosting

## 2.3 Bagging-based method
- Bagging: Bagging predictor | 1996 
- Diversity Analysis on Imbalanced Data Sets by Using Ensemble Models | 2009 |

## 2.4 Cost-sensitive ensemble
- AdaCost: Misclassification Cost-sensitive boosting
- AdaUBoost: AdaBoost with Unequal loss functions
- AsymBoost: Asymmetric AdaBoost and detector cascade

# 3 Data Resampling
## 3.1 Over-sampling

## 3.2 Under-sampling

## 3.3 Hybrid-sampling

# 4 Cost-sensitive Learning


# 5 Deep Learning
## 5.1 Hard example mining

## 5.2 Loss function engineering











# Reference
- https://github.com/ZhiningLiu1998/awesome-imbalanced-learning
- https://github.com/xialeiliu/Awesome-LongTailed-Recognition


